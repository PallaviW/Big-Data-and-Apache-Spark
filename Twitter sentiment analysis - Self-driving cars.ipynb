{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Twitter sentiments for Self-driving cars\n",
    "\n",
    "Based on given train data, we need to classify tweets about driverless cars as from very positive to very negative. \n",
    "\n",
    "5 - very positive\n",
    "\n",
    "4 - slightly positive\n",
    "\n",
    "3 - neutral\n",
    "\n",
    "2 - slightly negative\n",
    "\n",
    "1 - very negative\n",
    "\n",
    "There are train and test datasets.\n",
    "\n",
    "#### Acknowledgements\n",
    "\n",
    "Thanks crowdflower.com, for providing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, Word2Vec\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "# these imports are how we build and manager our data science processes: cleaning data, preparing a model,\n",
    "# executing the model, and evaluating the model.\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import functools\n",
    "%matplotlib inline\n",
    "\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "## constants\n",
    "DATA_NAME = \"/home/ds/notebooks/datasets/Twitter_data/train.csv\"\n",
    "APP_NAME = \"Sentiment Analysis with tweets\"\n",
    "SPARK_URL = \"local[*]\"\n",
    "RANDOM_SEED = 141107\n",
    "TRAINING_DATA_RATIO = 0.8\n",
    "RF_NUM_TREES = 200\n",
    "RF_MAX_DEPTH = 4\n",
    "RF_NUM_BINS = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkSession.builder.appName(APP_NAME).master(SPARK_URL).getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data = sqlContext.read.csv(DATA_NAME, header=True, mode=\"DROPMALFORMED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|sentiment|                text|\n",
      "+---------+--------------------+\n",
      "|        5|@manjulamartin @K...|\n",
      "|        5|I want a Google d...|\n",
      "|        5|@Oatmeal @google ...|\n",
      "|        5|SO MUCH AWESOME! ...|\n",
      "|        5|@google is making...|\n",
      "|        5|You could call a ...|\n",
      "|        5|Ì¢‰âÂÒ@Marketpla...|\n",
      "|        5|Driverless taxis ...|\n",
      "|        5|This whole @googl...|\n",
      "|        5|Google's New Driv...|\n",
      "|        5|Riding in a drive...|\n",
      "|        5|This is the futur...|\n",
      "|        5|@NicoleLapin The ...|\n",
      "|        5|This is why we ne...|\n",
      "|        5|Google developed ...|\n",
      "|        5|@WOKVNews @jax_fl...|\n",
      "|        5|They're coming ou...|\n",
      "|        5|I want a driverle...|\n",
      "|        5|Driverless cars c...|\n",
      "|        5|#SKYNET Ì¢‰âÂÒ@S...|\n",
      "+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sentiment: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "967"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data = tweets_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "967"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As data analysis shows that there are no null records also data is in texual format we can not perform prediction modelling on data unless we convert that data to numeric values. we have to extract features from text data to analyze it. Lets vizualize data first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql.types import IntegerType\n",
    "#tweets_data = tweets_data.withColumn(\"sentiment\", tweets_data[\"sentiment\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "967"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to handle sentiments\n",
    "def tweeter_sentiment(x):\n",
    "    if x > 3:  #sentiments with very positive, slightly positive\n",
    "        return 2\n",
    "    elif x < 3: #sentiments with slightly negative, very negative\n",
    "        return 1\n",
    "    else:\n",
    "        return 0 #sentiments with neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_udf = udf(tweeter_sentiment, IntegerType())\n",
    "tweets_data = tweets_data.withColumn('sentiment_encoded',func_udf(tweets_data['sentiment'].cast(IntegerType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------------+\n",
      "|sentiment|                text|sentiment_encoded|\n",
      "+---------+--------------------+-----------------+\n",
      "|        5|@manjulamartin @K...|                2|\n",
      "|        5|I want a Google d...|                2|\n",
      "|        5|@Oatmeal @google ...|                2|\n",
      "|        5|SO MUCH AWESOME! ...|                2|\n",
      "|        5|@google is making...|                2|\n",
      "+---------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|sentiment_encoded|count|\n",
      "+-----------------+-----+\n",
      "|                1|  138|\n",
      "|                2|  234|\n",
      "|                0|  595|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "statuses = tweets_data.groupBy('sentiment_encoded').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(sentiment='3', count=595), Row(sentiment='5', count=57), Row(sentiment='1', count=23), Row(sentiment='4', count=177), Row(sentiment='2', count=115)]\n",
      "['3', '5', '1', '4', '2']\n",
      "['neutral', 'very positive', 'very negative', 'slightly positive', 'slightly negative']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x7f0fb2337c88>,\n",
       "  <matplotlib.axis.XTick at 0x7f0fcd4e89b0>,\n",
       "  <matplotlib.axis.XTick at 0x7f0fbc9a5400>,\n",
       "  <matplotlib.axis.XTick at 0x7f0fb1a6cd30>,\n",
       "  <matplotlib.axis.XTick at 0x7f0fb19f43c8>],\n",
       " <a list of 5 Text xticklabel objects>)"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA34AAAE/CAYAAAAZshH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3X28bXVdL/rPVxE0RR63pDyEKdXlnhJtH8OnIilTfIBTqJUaEPfszn2Z1qlOWfeWD7fOpdstk7zHe0iPYKmA5APXyCQULfMBUMRHghACQiEElIgU/d4/5m/FZLv23gv2mnvtNfb7/Xqt1xzjN35zjO/c+7fGGp85xhyzujsAAABM1/3WugAAAAAWS/ADAACYOMEPAABg4gQ/AACAiRP8AAAAJk7wAwAAmDjBD4DJqKrTq+q3q+opVXX5Kq73L6rqhDF9YlX9zSqu+wVV9d7VWh8ALGe3tS4AAFZbd/91ku/eVr+qekWSR3f3C7exvmesRl1VdWiSLyR5QHffNdb95iRvXo31A8CWOOMHAFtQM/5WArDu+WMGwLpVVY+tqo9X1Ver6qwkDxztR1XVdXP9fq2qrh/9Lq+qo6vq6Ul+I8nzq+r2qvrk6HthVf1OVX0oyR1JvnO0/S/33HS9tqpuq6rPV9XRcwuurqofmZt/RVX96Zj94Hi8dWzzCZtfOlpVT6yqi8a6L6qqJ84tu7Cq/o+q+tB4Le+tqv1X698TgOkS/ABYl6pq9yTvTPInSfZN8rYkP7FMv+9O8vNJ/n1375nkx5Jc3d3vSfJfk5zV3Q/p7sfMPe1FSTYl2TPJNcts/geS/H2S/ZO8PMnbq2rfFZT9g+Nx77HND29W675J/jzJqUn2S/IHSf68qvab6/bTSU5K8rAkuyf5lRVsF4BdnOAHwHp1ZJIHJPnD7v56d5+T5KJl+n0jyR5JDq+qB3T31d3999tY9+nd/Znuvqu7v77M8hvntntWksuTPHM7XsuSZya5orv/ZGz7rUk+n+TZc33e2N1/193/kuTsJEeswnYBmDjBD4D16hFJru/unmv7lrNz3X1lkl9M8ookN1bVmVX1iG2s+9ptLF9uu9ta50o8It/6Gq5JcuDc/Bfnpu9I8pBV2C4AEyf4AbBe3ZDkwKqqubZDluvY3W/p7icn+Y4kneR3lxZtYd1bal+y3Hb/cUz/c5Jvm1v27fdivf84apx3SJLrt/E8ANgqwQ+A9erDSe5K8tKqekBV/XiSx2/eqaq+u6qeWlV7JLkzyb8k+eZY/KUkh96HO3c+bG67z03yPyU5byy7NMlPjmUbkxw/97ybxra/cwvrPS/Jd1XVT1fVblX1/CSHJ3n3vawPAO5B8ANgXeruryX58SQnJvlykucnefsyXfdIckqSf8rsMsmHJfn1sext4/Hmqvr4vdj8R5McNtb5O0mO7+6bx7LfTPKoJLckeWWSt8zVfMfo/6GqurWqjtzsNd2c5FlJfjnJzUl+Ncmzuvuf7kVtAPAt6p4fUQAAAGBqnPEDAACYOMEPAABg4gQ/AACAiRP8AAAAJk7wAwAAmLjd1rqA7bH//vv3oYceutZlAAAArIlLLrnkn7p7w7b6revgd+ihh+biiy9e6zIAAADWRFVds5J+LvUEAACYOMEPAABg4gQ/AACAiRP8AAAAJk7wAwAAmLiFBr+q2ruqzqmqz1fV56rqCVW1b1WdX1VXjMd9Rt+qqlOr6sqquqyqHrfI2gAAAHYViz7j95ok7+nu70nymCSfS/KyJBd092FJLhjzSfKMJIeNn01JXrfg2gAAAHYJCwt+VbVXkh9M8oYk6e6vdfetSY5NcsbodkaS48b0sUne1DMfSbJ3VT18UfUBAADsKhZ5xu+RSW5K8saq+kRVvb6qHpzkgO6+YfT5YpIDxvSBSa6de/51ow0AAIDtsMjgt1uSxyV5XXc/Nsk/5+7LOpMk3d1J+t6stKo2VdXFVXXxTTfdtGrFAgAATNUig991Sa7r7o+O+XMyC4JfWrqEczzeOJZfn+TguecfNNruobtP6+6N3b1xw4YNCyseAABgKnZb1Iq7+4tVdW1VfXd3X57k6CSfHT8nJDllPL5rPOXcJD9fVWcm+YEkt81dErqu1CtrrUvY4frl9+rELQAAsAMtLPgNL0ny5qraPclVSU7K7Czj2VV1cpJrkjxv9D0vyTFJrkxyx+gLAADAdlpo8OvuS5NsXGbR0cv07SQvXmQ9AAAAu6JFf48fAAAAa0zwAwAAmDjBDwAAYOIEPwAAgIkT/AAAACZO8AMAAJg4wQ8AAGDiBD8AAICJE/wAAAAmTvADAACYOMEPAABg4gQ/AACAiRP8AAAAJk7wAwAAmDjBDwAAYOIEPwAAgIkT/AAAACZO8AMAAJg4wQ8AAGDiBD8AAICJE/wAAAAmTvADAACYOMEPAABg4gQ/AACAiRP8AAAAJk7wAwAAmDjBDwAAYOIEPwAAgIkT/AAAACZO8AMAAJg4wQ8AAGDiBD8AAICJE/wAAAAmTvADAACYuIUGv6q6uqo+VVWXVtXFo23fqjq/qq4Yj/uM9qqqU6vqyqq6rKoet8jaAAAAdhU74ozfD3f3Ed29ccy/LMkF3X1YkgvGfJI8I8lh42dTktftgNoAAAAmby0u9Tw2yRlj+owkx821v6lnPpJk76p6+BrUBwAAMCmLDn6d5L1VdUlVbRptB3T3DWP6i0kOGNMHJrl27rnXjTYAAAC2w24LXv+Tu/v6qnpYkvOr6vPzC7u7q6rvzQpHgNyUJIcccsjqVQoAADBRCz3j193Xj8cbk7wjyeOTfGnpEs7xeOPofn2Sg+eeftBo23ydp3X3xu7euGHDhkWWDwAAMAkLC35V9eCq2nNpOsnTknw6yblJThjdTkjyrjF9bpKfGXf3PDLJbXOXhAIAAHAfLfJSzwOSvKOqlrbzlu5+T1VdlOTsqjo5yTVJnjf6n5fkmCRXJrkjyUkLrA0AAGCXsbDg191XJXnMMu03Jzl6mfZO8uJF1QMAALCrWouvcwAAAGAHEvwAAAAmTvADAACYOMEPAABg4gQ/AACAiRP8AAAAJk7wAwAAmDjBDwAAYOIEPwAAgIkT/AAAACZO8AMAAJg4wQ8AAGDiBD8AAICJE/wAAAAmTvADAACYOMEPAABg4gQ/AACAiRP8AAAAJk7wAwAAmDjBDwAAYOIEPwAAgIkT/AAAACZO8AMAAJg4wQ8AAGDiBD8AAICJE/wAAAAmTvADAACYOMEPAABg4gQ/AACAiRP8AAAAJk7wAwAAmDjBDwAAYOIEPwAAgIkT/AAAACZO8AMAAJi4hQe/qrp/VX2iqt495h9ZVR+tqiur6qyq2n207zHmrxzLD110bQAAALuCHXHG7xeSfG5u/neTvLq7H53kliQnj/aTk9wy2l89+gEAALCdFhr8quqgJM9M8voxX0memuSc0eWMJMeN6WPHfMbyo0d/AAAAtsOiz/j9YZJfTfLNMb9fklu7+64xf12SA8f0gUmuTZKx/LbRHwAAgO2wsOBXVc9KcmN3X7LK691UVRdX1cU33XTTaq4aAABgkhZ5xu9JSZ5TVVcnOTOzSzxfk2Tvqtpt9DkoyfVj+vokByfJWL5Xkps3X2l3n9bdG7t744YNGxZYPgAAwDQsLPh1969390HdfWiSn0zyvu5+QZL3Jzl+dDshybvG9LljPmP5+7q7F1UfAADArmItvsfv15L8UlVdmdln+N4w2t+QZL/R/ktJXrYGtQEAAEzObtvusv26+8IkF47pq5I8fpk+dyZ57o6oBwAAYFeyFmf8AAAA2IEEPwAAgIkT/AAAACZO8AMAAJg4wQ8AAGDiBD8AAICJE/wAAAAmTvADAACYOMEPAABg4gQ/AACAiRP8AAAAJk7wAwAAmDjBDwAAYOIEPwAAgIkT/AAAACZO8AMAAJg4wQ8AAGDiBD8AAICJE/wAAAAmbkXBr6p+oaoeWjNvqKqPV9XTFl0cAAAA22+lZ/x+tru/kuRpSfZJ8qIkpyysKgAAAFbNSoNfjcdjkvxJd39mrg0AAICd2EqD3yVV9d7Mgt9fVtWeSb65uLIAAABYLbutsN/JSY5IclV331FV+yU5aXFlAQAAsFpWesbv/O7+eHffmiTdfXOSVy+uLAAAAFbLVs/4VdUDk3xbkv2rap/c/bm+hyY5cMG1AQAAsAq2dannzyX5xSSPSHJJ7g5+X0ny2gXWBQAAwCrZavDr7tckeU1VvaS7/2gH1QQAAMAqWtHNXbr7j6rqiUkOnX9Od79pQXUBAACwSlYU/KrqT5I8KsmlSb4xmjuJ4AcAALCTW+nXOWxMcnh39yKLAQAAYPWt9OscPp3k2xdZCAAAAIux0jN++yf5bFV9LMm/LjV293MWUhUAAACrZqXB7xWLLAIAAIDFWeldPT9wb1c8vvz9g0n2GNs5p7tfXlWPTHJmkv0y+27AF3X316pqj8xuFvP9SW5O8vzuvvrebhcAAIB7WtFn/Krqq1X1lfFzZ1V9o6q+so2n/WuSp3b3Y5IckeTpVXVkkt9N8urufnSSW5KcPPqfnOSW0f7q0Q8AAIDttKLg1917dvdDu/uhSR6U5CeS/LdtPKe7+/Yx+4Dx00memuSc0X5GkuPG9LFjPmP50VVVK30hAAAALG+ld/X8NyPQvTPJj22rb1Xdv6ouTXJjkvOT/H2SW7v7rtHluiQHjukDk1w7tnFXktsyuxwUAACA7bDSL3D/8bnZ+2X2vX53but53f2NJEdU1d5J3pHke+5LkZvVsinJpiQ55JBDtnd1AAAAk7fSu3o+e276riRXZ3Zp5op0961V9f4kT0iyd1XtNs7qHZTk+tHt+iQHJ7muqnZLsldmN3nZfF2nJTktSTZu3OgL5QEAALZhpXf1POnerriqNiT5+gh9D0ryo5ndsOX9SY7P7M6eJyR513jKuWP+w2P5+7pbsAMAANhOK72r50FV9Y6qunH8/FlVHbSNpz08yfur6rIkFyU5v7vfneTXkvxSVV2Z2Wf43jD6vyHJfqP9l5K87L68IAAAAO5ppZd6vjHJW5I8d8y/cLT96Jae0N2XJXnsMu1XJXn8Mu13zq0fAACAVbLSu3pu6O43dvdd4+f0JBsWWBcAAACrZKXB7+aqeuH4eob7V9ULs8yNVwAAANj5rDT4/WyS5yX5YpIbMrv5yokLqgkAAIBVtNLP+L0qyQndfUuSVNW+Sf7vzAIhAAAAO7GVnvH7vqXQlyTd/eUsc+MWAAAAdj4rDX73q6p9lmbGGb+Vni0EAABgDa00vP1+kg9X1dvG/HOT/M5iSgIAAGA1rSj4dfebquriJE8dTT/e3Z9dXFkAAACslhVfrjmCnrAHAACwzqz0M34AAACsU4IfAADAxAl+AAAAEyf4AQAATJzgBwAAMHGCHwAAwMQJfgAAABMn+AEAAEyc4AcAADBxgh8AAMDECX4AAAATJ/gBAABMnOAHAAAwcYIfAADAxAl+AAAAEyf4AQAATJzgBwAAMHGCHwAAwMQJfgAAABMn+AEAAEyc4AcAADBxgh8AAMDECX4AAAATJ/gBAABMnOAHAAAwcYIfAADAxC0s+FXVwVX1/qr6bFV9pqp+YbTvW1XnV9UV43Gf0V5VdWpVXVlVl1XV4xZVGwAAwK5kkWf87kryy919eJIjk7y4qg5P8rIkF3T3YUkuGPNJ8owkh42fTUlet8DaAAAAdhkLC37dfUN3f3xMfzXJ55IcmOTYJGeMbmckOW5MH5vkTT3zkSR7V9XDF1UfAADArmKHfMavqg5N8tgkH01yQHffMBZ9MckBY/rAJNfOPe260QYAAMB2WHjwq6qHJPmzJL/Y3V+ZX9bdnaTv5fo2VdXFVXXxTTfdtIqVAgAATNNCg19VPSCz0Pfm7n77aP7S0iWc4/HG0X59koPnnn7QaLuH7j6tuzd298YNGzYsrngAAICJWORdPSvJG5J8rrv/YG7RuUlOGNMnJHnXXPvPjLt7HpnktrlLQgEAALiPdlvgup+U5EVJPlVVl46230hySpKzq+rkJNcked5Ydl6SY5JcmeSOJCctsDYAAIBdxsKCX3f/TZLawuKjl+nfSV68qHoAAAB2VTvkrp4AAACsHcEPAABg4gQ/AACAiRP8AAAAJk7wAwAAmLhFfp0DAABbUa/c0g3Qp6tf3mtdAuySnPEDAACYOMEPAABg4gQ/AACAiRP8AAAAJk7wAwAAmDjBDwAAYOIEPwAAgIkT/AAAACZO8AMAAJg4wQ8AAGDiBD8AAICJE/wAAAAmTvADAACYOMEPAABg4gQ/AACAiRP8AAAAJk7wAwAAmDjBDwAAYOIEPwAAgIkT/AAAACZO8AMAAJg4wQ8AAGDiBD8AAICJE/wAAAAmTvADAACYOMEPAABg4nZb6wIAAID15cILa61L2OGOOqrXuoTt4owfAADAxC0s+FXV/6iqG6vq03Nt+1bV+VV1xXjcZ7RXVZ1aVVdW1WVV9bhF1QUAALCrWeQZv9OTPH2ztpcluaC7D0tywZhPkmckOWz8bEryugXWBQAAsEtZWPDr7g8m+fJmzccmOWNMn5HkuLn2N/XMR5LsXVUPX1RtAAAAu5Id/Rm/A7r7hjH9xSQHjOkDk1w71++60QYAAMB2WrObu3R3J7nXt8apqk1VdXFVXXzTTTctoDIAAIBp2dHB70tLl3COxxtH+/VJDp7rd9Bo+xbdfVp3b+zujRs2bFhosQAAAFOwo4PfuUlOGNMnJHnXXPvPjLt7HpnktrlLQgEAANgOC/sC96p6a5KjkuxfVdcleXmSU5KcXVUnJ7kmyfNG9/OSHJPkyiR3JDlpUXUBAADsahYW/Lr7p7aw6Ohl+naSFy+qFgAAgF3Zmt3cBQAAgB1D8AMAAJg4wQ8AAGDiBD8AAICJE/wAAAAmTvADAACYOMEPAABg4gQ/AACAiRP8AAAAJk7wAwAAmDjBDwAAYOIEPwAAgIkT/AAAACZO8AMAAJi43da6AGDnceGFtdYl7HBHHdVrXQIAwMI54wcAADBxgh8AAMDECX4AAAATJ/gBAABMnOAHAAAwcYIfAADAxAl+AAAAEyf4AQAATJzgBwAAMHGCHwAAwMQJfgAAABMn+AEAAEyc4AcAADBxgh8AAMDE7bbWBQDA9qoLL1zrEna4PuqotS4BgHXEGT8AAICJE/wAAAAmTvADAACYOMEPAABg4gQ/AACAidupgl9VPb2qLq+qK6vqZWtdDwAAwBTsNMGvqu6f5P9J8owkhyf5qao6fG2rAgAAWP92muCX5PFJruzuq7r7a0nOTHLsGtcEAACw7u1Mwe/AJNfOzV832gAAANgOu611AfdWVW1KsmnM3l5Vl69lPTuh/ZP8047eaL2idvQmmZ41GbuJsct2W5v97o7eIFOzRvtcxwxstzUbuzvxnvc7VtJpZwp+1yc5eG7+oNF2D919WpLTdlRR601VXdzdG9e6Dri3jF3WK2OX9ci4Zb0ydu+7nelSz4uSHFZVj6yq3ZP8ZJJz17gmAACAdW+nOePX3XdV1c8n+csk90/yP7r7M2tcFgAAwLq30wS/JOnu85Kct9Z1rHMug2W9MnZZr4xd1iPjlvXK2L2PqrvXugYAAAAWaGf6jB8AAAALIPhNUFUdWlU/fR+fe/tq1wOrraqeU1UvG9PHVdXhc8teVVU/snbVwb1TVb+x2fzfrlUt7BhVdWFVbRzT51XV3ivtv1n7EVV1zNz8iVX12tWveOuWxuzmxx9VtbGqTt3R9bD6pjZmt2bKxxWC3zQdmmTZ4FdVO9XnOpmGmtlh+5PuPre7TxmzxyU5fG7Zb3X3X+2oWlh/dvR4XYF7BL/ufuJaFcKO193HdPet9/HpRyQ5Zpu9FmxuzB6aueOP7r64u1+6JkWxMFMYs9sw2eOKnekP3y5vvFP2uar646r6TFW9t6oeVFWPqqr3VNUlVfXXVfU9o//pVXX83POXztadkuQpVXVpVf3n8W7KuVX1viQXVNVDquqCqvp4VX2qqo5dg5fLTqaqTqmqF8/Nv6KqfmVM/5equqiqLquqV462Q6vq8qp6U5JPJ/nNqvrDuef/x6p69TLbub2qXj3G+AVVtWG0H1FVHxnbeEdV7TPaX1pVnx3tZ462E6vqtVX1xCTPSfJ7Y7w/aun3oqqeXlVvm9vuUVX17jH9tKr68PgdeFtVPWT1/0VZpB08Xn+nqj45xucBo31DVf3Z2M5FVfWkufbzx/h+fVVdU1X7j2XvHPvxz1TVpqXXkeRBY/y+eWmb4/HMqnrmXC1LY/v+VfV7c6/x51b5n5dVUlUPrqo/H+Pn01X1/GX6XD03Rn5zjNO/qaq3Lo3p4blV9bGq+ruqekrNvvrqVUmeP8bP8+fWuWdVfaGqHjDmHzo/P9fv9Kr6f6vq4rHeZ432B1bVG8cxwieq6odH+/88arh0jL3DRvuWjj+Oqqp3V9X9xuvce27bV1TVAVv6XWJtrJMxe2pV/W1VXVX3PA7+ln3/1moc+/2Lxmv9s6r6tpr6cUV3+9lJfjJ7p+yuJEeM+bOTvDDJBUkOG20/kOR9Y/r0JMfPPf/28XhUknfPtZ+Y5Lok+4753ZI8dEzvn+TK3H2jn9vX+t/Bz5qNv8cm+cDc/GeTHJzkaZndQasye7Po3Ul+cIzXbyY5cvR/SJK/T/KAMf+3Sb53me10kheM6d9K8toxfVmSHxrTr0ryh2P6H5PsMab3Ho8nzj1v89+D05McP8b5PyR58Gh/3fh92j/JB+fafy3Jb631v7+fnXq8PntM/19J/vcx/ZYkTx7ThyT53Jh+bZJfH9NPH8/ff8wv7YMflFn43G/M377ZNpf25f8hyRljevck147nbpqrY48kFyd55Fr/n/hZdpz+RJI/npvfazxemGTjmL567Jf+fZJLkzwwyZ5JrkjyK3P9f39MH5Pkr8b0v+0LN59P8sYkx43pTUvP36y+05O8Z/yuHJbZscIDk/xyZl+rlSTfM/alD0zyR7l7/717kgdtNmaPyj2PP/5tPslrkpw0pn9g7jUs+7vkx5jdyph92xizhye5crRvad+/tRr3m1vvbyd5ydw2Jnlc4bK/nc8XuvvSMX1JZgcrT0zytqpa6rPHfVjv+d395TFdSf5rVf1gZgdCByY5IMkX72vRrH/d/YmqelhVPSLJhiS3dPe1VfULme1QPzG6PiSzA4R/SHJNd39kPP/2mp1VflZVfS6zA+pPLbOpbyY5a0z/aZK3V9VemYW6D4z2MzLbsSezQPjmqnpnknfei9dzV1W9J8mzq+qcJM9M8qtJfiizPxYfGr9Tuyf58ErXy85hB47Xr2V2AJHM9sk/OqZ/JMnhc/vlh453eJ+cWWBLd7+nqm6ZW9dLq+o/jOmDR103b+Vl/kWS11TVHpmFyA92979U1dOSfN/cO917jXV9YSvrYm18KsnvV9XvZhaA/norfZ+U5F3dfWeSO6vq/9ts+dvH49Kxwba8PrN93juTnJTkP26h39nd/c0kV1TVVZkFvSdnFvLS3Z+vqmuSfFdm+8r/raoOSvL27r5iBXUsOSuzN/vemOQnc/ffgWV/l7rbPQfWxnoYs+8cY/azNa7CyGy/v9y+f8+t1Pjvquq3k+w9+v/l1oqbwnGF4Lfz+de56W9kFshu7e4jlul7V8blujX7vMruW1nvP89NvyCzA6Xv7+6vV9XVmb0TAm/L7F2tb8/df5Qryf/Z3f99vmNVHZp7jqtkttP+jSSfz+yP+0ps6ztlnpnZu3bPzuyA43tXuN4kOTPJzyf5cpKLu/urNdsrn9/dP3Uv1sPOaUeM16/3eAs3s33y0t/N+2V29vDOzbaz7Eqq6qjMDnCf0N13VNWF2cZ+t7vvHP1+LMnzMxvPyew1vqS7t3qQwtrr7r+rqsdldsbjt6vqgu5+1X1c3dLxwfw43Nq2P1SzS5yPSnL/7v70lrpuY35+nW+pqo9mtl8+r6p+rrvft+3Sk8wOhB9ds8v7j8vsDEuyhd8l1sY6GbPzx8o197jcvv8Xt7LJ0zM7w/jJqjoxszPU27Kujyt8xm/n95UkX6iq5yb/dlOCx4xlVyf5/jH9nCRL10F/NbN3OLZkryQ3jtD3w0m+Y9WrZr06K7N3Yo/P3Wfc/jLJzy5dr15VB1bVw5Z7cnd/NLMzGT+d5K1b2Mb9xvoz+v1Nd9+W5Jaqespof1GSD4w3NA7u7vdndunEXpm9Kzdva+P9A0kel9m7hksHzR9J8qSqevR4PQ+uqu/awvPZue2I8bol703ykqWZqlp6c+5DSZ432p6WZJ/RvldmZyXvqNnntI+cW9fXN/8cy5yzMnvn+ymZXZKXzF7j/zr3WZjvqqoH38v62QHGGek7uvtPk/xeZvujLflQZmcSHjjG77NWsIlt/b1/U2aXUm7tjbjn1uwzeI9K8p1JLk/y15m9SZyxfzwkyeVV9Z1JruruU5O8K8n3rbSe8QbKO5L8QWaXcy6d7d7S7xJrYJ2M2eVsad+/tRr3THLD2Je+YIU1ruvjCsFvfXhBkpOr6pNJPpNk6WYsf5zkh0b7E3L3u9mXJflGzT6s+p+XWd+bk2ysqk8l+ZnM3u2GdPdnMtvZXd/dN4y292a2E/7wGDPnZOs77bOTfKi7b9nC8n9O8viq+nSSp2b2eb4kOSGzD1Nfltldv16V5P5J/nRs9xNJTu1vvZPYmUn+S81uQPCozV7PNzK7TO8Z4zHdfVNmnyl469jWhzO7tIl1ZgeN1y15aWb70cuq6rNJ/tNof2WSp43x/dzMLqH/amahbbdxWekpmR0oLDktyWU1bu6ymfdmdhnRX3X310bb6zP7TOPHx3b+e1zBs7P63iQfq6pLk7w8d5/l+hbdfVGSczP7G/4XmV1yd9s21v/+zC6TvMeNMua8ObM3H7b2xsY/JPnY2OZ/Gmfe/luS+43fobOSnNjd/5rZmxqfHq/n32V2kD5vW8cfZ2X2maiz5tq29LvE2lgPY3a5Wpbd92+jxt9M8tHMwuH8sfBkjyuWbugBsCpqdoerV3f3BVtYfnt375x3u2KXs63xeh/Wt0eSb4zPgjwhyeu2cKk+fIulz7ZV1bdldrOITd398e1Y3/FJju3uF21h+el4ZezOAAAAiElEQVSZfY7rnPu6DXZtO3rM7gw1rmfeIQRWRc1u0/2xJJ9crYNoWJQFjtdDkpw9LlP+WrZ8cwJYzmk1++LoB2Z2R9ftOYD+o8zOSuzs35nG+rYexuyq1bjeOeMHAAAwcT7jBwAAMHGCHwAAwMQJfgAAABMn+AEAAEyc4AcAADBxgh8AAMDE/f+ir2ylmGHvcgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0fb1f8d9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = (15, 5)\n",
    " \n",
    "#statuses = tweets_data.groupBy('sentiment_encoded').count().collect()\n",
    "statuses = tweets_data.groupBy('sentiment').count().collect()\n",
    "print(statuses)\n",
    "categories = [i[0] for i in statuses]\n",
    "counts = [i[1] for i in statuses]\n",
    "display_label = []\n",
    "\n",
    "#for item in categories:\n",
    "    #if item == 1:\n",
    "        #display_label.append('negative')\n",
    "    #elif item == 2:\n",
    "       # display_label.append('positive')\n",
    "    #else:\n",
    "        #display_label.append('neutral')\n",
    "print(categories)\n",
    "for item in categories:\n",
    "    if item == '5':\n",
    "        display_label.append('very positive')\n",
    "    elif item == '4':\n",
    "        display_label.append('slightly positive')\n",
    "    elif item == '3':\n",
    "        display_label.append('neutral')\n",
    "    elif item == '2':\n",
    "        display_label.append('slightly negative')\n",
    "    else:\n",
    "        display_label.append('very negative')\n",
    "        \n",
    "print(display_label)        \n",
    "ind = np.array(range(len(categories)))\n",
    "width = 0.35\n",
    "plt.bar(ind, counts, width=width, color='gyc')\n",
    " \n",
    "plt.ylabel('counts')\n",
    "plt.title('distribution')\n",
    "plt.xticks(ind, display_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph shows that sentiment for self driving car tweets is more of neutral compared to positive or negative. Lets find out some features for predictive modelling on the twitter data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try different techniques for feature engineering like word2vec or Tf/Idf to make data ready for predictive modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = Tokenizer(inputCol=\"text\", outputCol=\"tokenized_text\").transform(tweets_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2Vec = Word2Vec(vectorSize=300, seed=42, inputCol=\"tokenized_text\", outputCol=\"w2v_vector\").fit(tokenized_data)\n",
    "w2vdf=word2Vec.transform(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sentiment: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- tokenized_text: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- w2v_vector: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w2vdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------------------+\n",
      "|sentiment|                text|      tokenized_text|          w2v_vector|\n",
      "+---------+--------------------+--------------------+--------------------+\n",
      "|        5|@manjulamartin @K...|[@manjulamartin, ...|[0.00195620217564...|\n",
      "|        5|I want a Google d...|[i, want, a, goog...|[0.01613896682101...|\n",
      "|        5|@Oatmeal @google ...|[@oatmeal, @googl...|[0.01142727420665...|\n",
      "|        5|SO MUCH AWESOME! ...|[so, much, awesom...|[0.00425823115898...|\n",
      "|        5|@google is making...|[@google, is, mak...|[0.00704800314916...|\n",
      "|        5|You could call a ...|[you, could, call...|[0.00506992266807...|\n",
      "|        5|Ì¢‰âÂÒ@Marketpla...|[ì¢‰ââò@marketpl...|[0.00591101296330...|\n",
      "|        5|Driverless taxis ...|[driverless, taxi...|[0.01109385670618...|\n",
      "|        5|This whole @googl...|[this, whole, @go...|[0.00681427969935...|\n",
      "|        5|Google's New Driv...|[google's, new, d...|[0.00581081371961...|\n",
      "|        5|Riding in a drive...|[riding, in, a, d...|[0.00700959832722...|\n",
      "|        5|This is the futur...|[this, is, the, f...|[0.00538454561716...|\n",
      "|        5|@NicoleLapin The ...|[@nicolelapin, th...|[0.00541953135398...|\n",
      "|        5|This is why we ne...|[this, is, why, w...|[0.00911802430346...|\n",
      "|        5|Google developed ...|[google, develope...|[0.01002116555658...|\n",
      "|        5|@WOKVNews @jax_fl...|[@wokvnews, @jax_...|[0.00388369919382...|\n",
      "|        5|They're coming ou...|[they're, coming,...|[0.01400958408339...|\n",
      "|        5|I want a driverle...|[i, want, a, driv...|[0.01214915075356...|\n",
      "|        5|Driverless cars c...|[driverless, cars...|[0.00659389075423...|\n",
      "|        5|#SKYNET Ì¢‰âÂÒ@S...|[#skynet, ì¢‰ââò...|[0.00504870647670...|\n",
      "+---------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w2vdf.select('sentiment','text','tokenized_text','w2v_vector').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF/IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"tokenized_text\", outputCol=\"tfidf_vector\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"tfidf_vector\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "tfidf_df = idfModel.transform(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|sentiment|                text|      tokenized_text|        tfidf_vector|            features|\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|        5|@manjulamartin @K...|[@manjulamartin, ...|(20,[0,2,3,4,6,7,...|(20,[0,2,3,4,6,7,...|\n",
      "|        5|I want a Google d...|[i, want, a, goog...|(20,[0,9,10,12,16...|(20,[0,9,10,12,16...|\n",
      "|        5|@Oatmeal @google ...|[@oatmeal, @googl...|(20,[0,1,3,4,9,10...|(20,[0,1,3,4,9,10...|\n",
      "|        5|SO MUCH AWESOME! ...|[so, much, awesom...|(20,[0,2,4,5,6,8,...|(20,[0,2,4,5,6,8,...|\n",
      "|        5|@google is making...|[@google, is, mak...|(20,[0,1,8,10,16,...|(20,[0,1,8,10,16,...|\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we tried different ways for feature extraction, lets try to perform random forest classifier on those datasets to see which one performs better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the training indexers/split data/classifier\n",
    "# first we'll generate a labelIndexer\n",
    "labelIndexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"indexedLabel\").fit(w2vdf)\n",
    "\n",
    "# now generate the indexed feature vector.\n",
    "featureIndexer = VectorIndexer(inputCol=\"w2v_vector\", outputCol=\"indexedFeatures\", maxCategories=5).fit(w2vdf)\n",
    "    \n",
    "# Split the data into training and validation sets (30% held out for testing)\n",
    "(trainingData, testData) = w2vdf.randomSplit([TRAINING_DATA_RATIO, 1 - TRAINING_DATA_RATIO])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=RF_NUM_TREES)\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline_4f788c08a0c66b66da07\n"
     ]
    }
   ],
   "source": [
    "print(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------------------+\n",
      "|sentiment|                text|      tokenized_text|          w2v_vector|\n",
      "+---------+--------------------+--------------------+--------------------+\n",
      "|        1|\"\"\"Driver-less ca...|[\"\"\"driver-less, ...|[0.00421577897903...|\n",
      "|        1|\"8:28 AM - The dr...|[\"8:28, am, -, th...|[0.00465906232737...|\n",
      "|        1|#Durham Driverles...|[#durham, driverl...|[0.00581674999848...|\n",
      "|        1|@d3signerd Ì¢‰âÂå...|[@d3signerd, ì¢‰â...|[0.00788420691969...|\n",
      "|        1|@drgitlin yes, I ...|[@drgitlin, yes,,...|[0.00661176691175...|\n",
      "|        1|@google watched t...|[@google, watched...|[0.00477988349040...|\n",
      "|        1|@littleben2009 no...|[@littleben2009, ...|[0.00787191899467...|\n",
      "|        1|Also, cars with d...|[also,, cars, wit...|[0.00346490017060...|\n",
      "|        1|Aren't they alrea...|[aren't, they, al...|[0.00503567502164...|\n",
      "|        1|CAR WARS - Action...|[car, wars, -, ac...|[0.00354269497936...|\n",
      "|        1|Driverless Cars: ...|[driverless, cars...|[0.00801549779716...|\n",
      "|        1|Driverless cars ?...|[driverless, cars...|[0.00912093909989...|\n",
      "|        1|Driverless cars a...|[driverless, cars...|[0.00590884130991...|\n",
      "|        1|Driverless cars. ...|[driverless, cars...|[0.00741107877984...|\n",
      "|        1|Good. I still won...|[good., i, still,...|[0.00425439513070...|\n",
      "|        1|Google developing...|[google, developi...|[0.01250464841723...|\n",
      "|        1|Google driver-les...|[google, driver-l...|[0.00219993525339...|\n",
      "|        1|So google has a d...|[so, google, has,...|[0.00447684908497...|\n",
      "|        1|Thanks for share....|[thanks, for, sha...|[0.00495200723697...|\n",
      "|        1|They're on about ...|[they're, on, abo...|[0.00695627317015...|\n",
      "+---------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------------------+\n",
      "|sentiment|                text|      tokenized_text|          w2v_vector|\n",
      "+---------+--------------------+--------------------+--------------------+\n",
      "|        1|@d3signerd Ì¢‰âÂå...|[@d3signerd, ì¢‰â...|[0.00788420691969...|\n",
      "|        1|Driverless cars a...|[driverless, cars...|[0.00590884130991...|\n",
      "|        2|\"Must develop pol...|[\"must, develop, ...|[0.00413121058954...|\n",
      "|        2|\"No it won't, bec...|[\"no, it, won't,,...|[0.00319409042342...|\n",
      "|        2|#hooli did it fir...|[#hooli, did, it,...|[0.00433981266715...|\n",
      "|        2|@MayorGregor, $40...|[@mayorgregor,, $...|[0.00358153765740...|\n",
      "|        2|@henebb #1 barrie...|[@henebb, #1, bar...|[0.00470253940233...|\n",
      "|        2|@jamesgleave1 @ec...|[@jamesgleave1, @...|[0.00530631520814...|\n",
      "|        2|@jeffmichael11 Dr...|[@jeffmichael11, ...|[0.00607798432107...|\n",
      "|        2|A couple of Congr...|[a, couple, of, c...|[0.00579209045099...|\n",
      "|        2|California: where...|[california:, whe...|[0.00477825637790...|\n",
      "|        2|Cannot jus trust ...|[cannot, jus, tru...|[0.00619110134293...|\n",
      "|        2|Could technology ...|[could, technolog...|[0.00427092261011...|\n",
      "|        2|Do I think the Go...|[do, i, think, th...|[0.00429549838320...|\n",
      "|        2|Driverless car ro...|[driverless, car,...|[0.00402234091921...|\n",
      "|        2|Driverless cars a...|[driverless, cars...|[0.00538671281537...|\n",
      "|        2|Driverless cars c...|[driverless, cars...|[0.00637379163526...|\n",
      "|        2|Driverless cars n...|[driverless, cars...|[0.00624371555750...|\n",
      "|        2|Driverless robo-c...|[driverless, robo...|[0.01008624087585...|\n",
      "|        2|Go read literally...|[go, read, litera...|[0.00245274318915...|\n",
      "+---------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+----------+\n",
      "|sentiment|          w2v_vector|         probability|prediction|\n",
      "+---------+--------------------+--------------------+----------+\n",
      "|        1|[0.01037475324701...|[0.71629940098517...|       0.0|\n",
      "|        1|[0.00720105778392...|[0.66811916099119...|       0.0|\n",
      "|        2|[0.00671476894058...|[0.63743900109494...|       0.0|\n",
      "|        2|[0.00600486050243...|[0.63892448862445...|       0.0|\n",
      "|        2|[0.00185882163568...|[0.53376686159270...|       0.0|\n",
      "|        2|[0.00180028920294...|[0.56942345221848...|       0.0|\n",
      "|        2|[0.00722995524993...|[0.52356475539369...|       0.0|\n",
      "|        2|[0.00470253940233...|[0.51974635715288...|       0.0|\n",
      "|        2|[0.00712844041408...|[0.64511807752796...|       0.0|\n",
      "|        2|[0.00341963747632...|[0.57343665183618...|       0.0|\n",
      "+---------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select('sentiment','w2v_vector','probability','prediction').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sentiment: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- tokenized_text: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- w2v_vector: vector (nullable = true)\n",
      " |-- indexedLabel: double (nullable = true)\n",
      " |-- indexedFeatures: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.394118\n",
      "Accuracy = 0.605882\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Test Error = {(1.0 - accuracy):g}\")\n",
    "print(f\"Accuracy = {accuracy:g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|sentiment|\n",
      "+---------+\n",
      "|        1|\n",
      "|        3|\n",
      "|        5|\n",
      "|        4|\n",
      "|        2|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select('sentiment').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['very negative',\n",
       " 'neutral',\n",
       " 'very positive',\n",
       " 'slightly positive',\n",
       " 'slightly negative']"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display_label.append('none')\n",
    "#display_label.remove('none')\n",
    "display_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mix of label input types (string and number)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-686-b50e2f152a7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m pd.DataFrame(confusion_matrix(predictions.select('sentiment').collect(), \n\u001b[0;32m----> 2\u001b[0;31m              predictions.select('prediction').collect()),\n\u001b[0m\u001b[1;32m      3\u001b[0m              \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisplay_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m              index= display_label)\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[0;34m(*ys)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;31m# Check that we don't mix string type with number type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mys_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mix of label input types (string and number)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Mix of label input types (string and number)"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(confusion_matrix(predictions.select('sentiment').collect(), \n",
    "             predictions.select('prediction').collect()),\n",
    "             columns = display_label, \n",
    "             index= display_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest with TF/IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the training indexers/split data/classifier\n",
    "# first we'll generate a labelIndexer\n",
    "labelIndexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"indexedLabel\").fit(tfidf_df)\n",
    "\n",
    "# now generate the indexed feature vector.\n",
    "featureIndexer = VectorIndexer(inputCol=\"tfidf_vector\", outputCol=\"indexedFeatures\", maxCategories=5).fit(tfidf_df)\n",
    "    \n",
    "# Split the data into training and validation sets (30% held out for testing)\n",
    "(trainingData_tf, testData_tf) = tfidf_df.randomSplit([TRAINING_DATA_RATIO, 1 - TRAINING_DATA_RATIO])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=RF_NUM_TREES)\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData_tf)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.418478\n",
      "Accuracy = 0.581522\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Test Error = {(1.0 - accuracy):g}\")\n",
    "print(f\"Accuracy = {accuracy:g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (6, 6), indices imply (5, 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   4621\u001b[0m                 blocks = [make_block(values=blocks[0],\n\u001b[0;32m-> 4622\u001b[0;31m                                      placement=slice(0, len(axes[0])))]\n\u001b[0m\u001b[1;32m   4623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mmake_block\u001b[0;34m(values, placement, klass, ndim, dtype, fastpath)\u001b[0m\n\u001b[1;32m   2956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2957\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfastpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, placement, ndim, fastpath)\u001b[0m\n\u001b[1;32m    119\u001b[0m                              'implies %d' % (len(self.values),\n\u001b[0;32m--> 120\u001b[0;31m                                              len(self.mgr_locs)))\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Wrong number of items passed 6, placement implies 5",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-584-b50e2f152a7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m              predictions.select('prediction').collect()),\n\u001b[1;32m      3\u001b[0m              \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisplay_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m              index= display_label)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m                 mgr = self._init_ndarray(data, index, columns, dtype=dtype,\n\u001b[0;32m--> 361\u001b[0;31m                                          copy=copy)\n\u001b[0m\u001b[1;32m    362\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_init_ndarray\u001b[0;34m(self, values, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_infer_to_datetimelike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   4629\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'values'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4630\u001b[0m         \u001b[0mtot_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4631\u001b[0;31m         \u001b[0mconstruction_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconstruction_error\u001b[0;34m(tot_items, block_shape, axes, e)\u001b[0m\n\u001b[1;32m   4606\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Empty data passed with indices specified.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4607\u001b[0m     raise ValueError(\"Shape of passed values is {0}, indices imply {1}\".format(\n\u001b[0;32m-> 4608\u001b[0;31m         passed, implied))\n\u001b[0m\u001b[1;32m   4609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (6, 6), indices imply (5, 5)"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(confusion_matrix(predictions.select('sentiment').collect(), \n",
    "             predictions.select('prediction').collect()),\n",
    "             columns = display_label, \n",
    "             index= display_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like Tf/idf data perform better with Random Forest compared to word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression with word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"w2v_vector\", labelCol=\"sentiment_encoded\",predictionCol=\"prediction\", maxIter=10,\n",
    "                        regParam=0.3, elasticNetParam=0.8, family=\"multinomial\")\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(trainingData)\n",
    "# Make predictions.\n",
    "predictions = lrModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.415789\n",
      "Accuracy = 0.584211\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"sentiment_encoded\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Test Error = {(1.0 - accuracy):g}\")\n",
    "print(f\"Accuracy = {accuracy:g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.415789\n",
      "Accuracy = 0.584211\n"
     ]
    }
   ],
   "source": [
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n",
    "             .build())\n",
    "# Create 5-fold CrossValidator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"sentiment_encoded\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "cv = CrossValidator(estimator=lr, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=evaluator, \\\n",
    "                    numFolds=5)\n",
    "cvModel = cv.fit(trainingData)\n",
    "\n",
    "predictions = cvModel.transform(testData)\n",
    "# Evaluate best model\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Test Error = {(1.0 - accuracy):g}\")\n",
    "print(f\"Accuracy = {accuracy:g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression with TF/IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"sentiment_encoded\",predictionCol=\"prediction\", maxIter=10,\n",
    "                        regParam=0.3, elasticNetParam=0.8, family=\"multinomial\")\n",
    "\n",
    "# Fit the model\n",
    "lrModel_tf = lr.fit(trainingData_tf)\n",
    "# Make predictions.\n",
    "predictions = lrModel_tf.transform(testData_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.374332\n",
      "Accuracy = 0.625668\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"sentiment_encoded\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Test Error = {(1.0 - accuracy):g}\")\n",
    "print(f\"Accuracy = {accuracy:g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.374332\n",
      "Accuracy = 0.625668\n"
     ]
    }
   ],
   "source": [
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n",
    "             .build())\n",
    "# Create 5-fold CrossValidator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"sentiment_encoded\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "cv = CrossValidator(estimator=lr, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=evaluator, \\\n",
    "                    numFolds=5)\n",
    "cvModel = cv.fit(trainingData_tf)\n",
    "\n",
    "predictions = cvModel.transform(testData_tf)\n",
    "# Evaluate best model\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Test Error = {(1.0 - accuracy):g}\")\n",
    "print(f\"Accuracy = {accuracy:g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          negative  positive  neutral\n",
       "negative       117         0        0\n",
       "positive        29         0        0\n",
       "neutral         41         0        0"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(confusion_matrix(predictions.select('sentiment_encoded').collect(), \n",
    "             predictions.select('prediction').collect()),\n",
    "             columns = display_label, \n",
    "             index= display_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes(labelCol=\"sentiment_encoded\", featuresCol=\"features\", smoothing=1)\n",
    "model = nb.fit(trainingData_tf)\n",
    "predictions_nb = model.transform(testData_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|sentiment|                text|sentiment_encoded|      tokenized_text|        tfidf_vector|            features|       rawPrediction|         probability|prediction|\n",
      "+---------+--------------------+-----------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|        1|@drgitlin yes, I ...|                1|[@drgitlin, yes,,...|(20,[0,2,5,6,7,8,...|(20,[0,2,5,6,7,8,...|[-40.669641631669...|[0.73570110633048...|       0.0|\n",
      "|        1|Aren't they alrea...|                1|[aren't, they, al...|(20,[0,4,5,7,8,10...|(20,[0,4,5,7,8,10...|[-29.485434725364...|[0.67578420332527...|       0.0|\n",
      "|        2|\"\"\"@verge: The FB...|                1|[\"\"\"@verge:, the,...|(20,[0,1,4,5,10,1...|(20,[0,1,4,5,10,1...|[-26.715340027602...|[0.65191949788588...|       0.0|\n",
      "|        2|.@danlyke working...|                1|[.@danlyke, worki...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|[-33.905181289413...|[0.85890897028200...|       0.0|\n",
      "|        2|@ESPN_Colin I'm s...|                1|[@espn_colin, i'm...|(20,[0,3,5,7,8,10...|(20,[0,3,5,7,8,10...|[-31.058141221656...|[0.60689450548847...|       0.0|\n",
      "+---------+--------------------+-----------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_nb.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.379679\n",
      "Accuracy = 0.620321\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"sentiment_encoded\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions_nb)\n",
    "\n",
    "print(f\"Test Error = {(1.0 - accuracy):g}\")\n",
    "print(f\"Accuracy = {accuracy:g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>116</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          negative  positive  neutral\n",
       "negative       116         1        0\n",
       "positive        28         0        1\n",
       "neutral         41         0        0"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(confusion_matrix(predictions_nb.select('sentiment_encoded').collect(), \n",
    "             predictions_nb.select('prediction').collect()),\n",
    "             columns = display_label, \n",
    "             index= display_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA with word2vec data and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(k=100, inputCol=\"w2v_vector\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(w2vdf)\n",
    "\n",
    "result = model.transform(w2vdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the training indexers / split data / classifier\n",
    "# first we'll generate a labelIndexer\n",
    "labelIndexer = StringIndexer(inputCol=\"sentiment_encoded\", outputCol=\"indexedLabel\").fit(result)\n",
    "\n",
    "# now generate the indexed feature vector\n",
    "featureIndexer = VectorIndexer(inputCol=\"pcaFeatures\", outputCol=\"indexedFeatures\", maxCategories=4).fit(result)\n",
    "    \n",
    "# Split the data into training and validation sets (30% held out for testing)\n",
    "(pca_trainingData, pca_testData) = result.randomSplit([TRAINING_DATA_RATIO, 1 - TRAINING_DATA_RATIO])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=RF_NUM_TREES)\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model.  This also runs the indexers.\n",
    "pca_model = pipeline.fit(pca_trainingData)\n",
    "# Make predictions.\n",
    "pca_predictions = pca_model.transform(pca_testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.352941\n",
      "Accuracy = 0.647059\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(pca_predictions)\n",
    "\n",
    "print(f\"Test Error = {(1.0 - accuracy):g}\")\n",
    "print(f\"Accuracy = {accuracy:g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>119</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          negative  positive  neutral\n",
       "negative       119         5        0\n",
       "positive        24         0        0\n",
       "neutral         37         2        0"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(confusion_matrix(pca_predictions.select('sentiment_encoded').collect(), \n",
    "             pca_predictions.select('prediction').collect()),\n",
    "             columns = display_label, \n",
    "             index= display_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA with tf/idf data and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(k=10, inputCol=\"tfidf_vector\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(tfidf_df)\n",
    "\n",
    "result = model.transform(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the training indexers / split data / classifier\n",
    "# first we'll generate a labelIndexer\n",
    "labelIndexer = StringIndexer(inputCol=\"sentiment_encoded\", outputCol=\"indexedLabel\").fit(result)\n",
    "\n",
    "# now generate the indexed feature vector\n",
    "featureIndexer = VectorIndexer(inputCol=\"pcaFeatures\", outputCol=\"indexedFeatures\", maxCategories=4).fit(result)\n",
    "    \n",
    "# Split the data into training and validation sets (30% held out for testing)\n",
    "(pca_trainingData_tf, pca_testData_tf) = result.randomSplit([TRAINING_DATA_RATIO, 1 - TRAINING_DATA_RATIO])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=RF_NUM_TREES)\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.373057\n",
      "Accuracy = 0.626943\n"
     ]
    }
   ],
   "source": [
    "# Train model.  This also runs the indexers.\n",
    "pca_model = pipeline.fit(pca_trainingData_tf)\n",
    "# Make predictions.\n",
    "pca_predictions_tf = pca_model.transform(pca_testData_tf)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(pca_predictions_tf)\n",
    "\n",
    "print(f\"Test Error = {(1.0 - accuracy):g}\")\n",
    "print(f\"Accuracy = {accuracy:g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA with tf/idf and Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|sentiment|                text|sentiment_encoded|      tokenized_text|        tfidf_vector|            features|         pcaFeatures|\n",
      "+---------+--------------------+-----------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|        1|\"\"\"Driver-less ca...|                1|[\"\"\"driver-less, ...|(20,[3,4,5,8,9,10...|(20,[3,4,5,8,9,10...|[2.86499975615702...|\n",
      "|        1|\"8:28 AM - The dr...|                1|[\"8:28, am, -, th...|(20,[0,1,4,5,8,9,...|(20,[0,1,4,5,8,9,...|[5.79787445571474...|\n",
      "|        1|#Durham Driverles...|                1|[#durham, driverl...|(20,[0,4,5,6,8,10...|(20,[0,4,5,6,8,10...|[2.45008422179210...|\n",
      "|        1|@drgitlin yes, I ...|                1|[@drgitlin, yes,,...|(20,[0,2,5,6,7,8,...|(20,[0,2,5,6,7,8,...|[2.92870603029767...|\n",
      "|        1|@google watched t...|                1|[@google, watched...|(20,[0,1,2,3,5,6,...|(20,[0,1,2,3,5,6,...|[4.88646903377261...|\n",
      "+---------+--------------------+-----------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pca_trainingData_tf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"sentiment_encoded\",predictionCol=\"prediction\", maxIter=10,\n",
    "                        regParam=0.3, elasticNetParam=0.8, family=\"multinomial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.38342\n",
      "Accuracy = 0.61658\n"
     ]
    }
   ],
   "source": [
    "# Train model.  This also runs the indexers.\n",
    "pca_model = lr.fit(pca_trainingData_tf)\n",
    "# Make predictions.\n",
    "pca_predictions = pca_model.transform(pca_testData_tf)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"sentiment_encoded\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(pca_predictions)\n",
    "\n",
    "print(f\"Test Error = {(1.0 - accuracy):g}\")\n",
    "print(f\"Accuracy = {accuracy:g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA with Word2Vec and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(k=10, inputCol=\"w2v_vector\", outputCol=\"pca_w2v_Features\")\n",
    "model = pca.fit(w2vdf)\n",
    "\n",
    "result = model.transform(w2vdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the training indexers / split data / classifier\n",
    "# first we'll generate a labelIndexer\n",
    "labelIndexer = StringIndexer(inputCol=\"sentiment_encoded\", outputCol=\"indexedLabel\").fit(result)\n",
    "\n",
    "# now generate the indexed feature vector\n",
    "featureIndexer = VectorIndexer(inputCol=\"pca_w2v_Features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(result)\n",
    "    \n",
    "# Split the data into training and validation sets (30% held out for testing)\n",
    "(pca_trainingData, pca_testData) = result.randomSplit([TRAINING_DATA_RATIO, 1 - TRAINING_DATA_RATIO])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=RF_NUM_TREES)\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.409326\n",
      "Accuracy = 0.590674\n"
     ]
    }
   ],
   "source": [
    "# Train model.  This also runs the indexers.\n",
    "pca_model = pipeline.fit(pca_trainingData)\n",
    "pca_predictions = pca_model.transform(pca_testData)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(pca_predictions)\n",
    "\n",
    "print(f\"Test Error = {(1.0 - accuracy):g}\")\n",
    "print(f\"Accuracy = {accuracy:g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>109</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          negative  positive  neutral\n",
       "negative       109         4        1\n",
       "positive        29         1        1\n",
       "neutral         42         4        2"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(confusion_matrix(pca_predictions.select('sentiment_encoded').collect(), \n",
    "             pca_predictions.select('prediction').collect()),\n",
    "             columns = display_label, \n",
    "             index= display_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
